\documentclass[11pt,dvipsnames,usenames,aspectratio=169]{beamer}  % Add handout to options to disable overlays

% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{%
  \usetheme{CambridgeUS}    % or try default, Darmstadt, Warsaw, ...
  \usecolortheme{whale}     % or try albatross, beaver, crane, ...
  \usefonttheme{serif}          % or try default, structurebold, ...
  % \usefonttheme[onlymath]{serif}
  % \setbeamertemplate{navigation symbols}{}
  % \setbeamercovered{transparent}

  \setbeamercolor{title}{fg=white}
  \setbeamerfont{title}{series=\bfseries}
  \setbeamercolor{frametitle}{fg=black}
  \setbeamerfont{frametitle}{series=\bfseries}

  \setbeamercolor{section in head/foot}{fg=white}
  \setbeamerfont{section in head/foot}{series=\bfseries}
  \setbeamercolor{subsection in head/foot}{fg=white}
  \setbeamerfont{subsection in head/foot}{series=\bfseries}
  \setbeamercolor{author in head/foot}{fg=white}
  \setbeamerfont{author in head/foot}{series=\bfseries}
  \setbeamercolor{title in head/foot}{fg=white}
  \setbeamerfont{title in head/foot}{series=\bfseries}

  \setbeamercolor{block title}{use=structure,fg=white,bg=title in head/foot.bg}
  \setbeamerfont{block title}{series=\bfseries}
  \setbeamercolor{block body}{use=structure,fg=black,bg=black!1!white}
}

% Support graying out frame elements
\newcommand{\FrameOpaque}{\setbeamercovered{again covered={\opaqueness<1->{40}}}}
% Transition slide
\newcommand{\transitionFrame}[1]{%
{%
  \begin{frame}[plain,noframenumbering]{}{} % the plain option removes the sidebar and header from the title page
    \setbeamertemplate{final page}[text]{\Large \textbf{#1}}
    \usebeamertemplate{final page}
  \end{frame}}
}

\usepackage{fontawesome}  % Used for defining the external hyperlink

% Imported via UltiSnips
\usepackage{graphicx} % Loading the package
\graphicspath{{img/}} % Setting the default folder containing the graphics

\usepackage[numbers]{natbib}
\usepackage{appendixnumberbeamer}

% \hypersetup{colorlinks=true,allcolors=blue}

% Here's where the presentation starts, with the info for the title slide
\title[Principles Explainable ML]{Principles and Practice of Explainable Machine Learning}
\author[Belle \& Papantonis]{%
  {Vaishak Belle}\inst{1}\inst{2}
  \and
  {Ioannis Papantonis}\inst{1}
}

\institute[U.\ Edinburgh]{%
  \textsuperscript{1}\textbf{University of Edinburgh}
  \and
  \textsuperscript{2}\textbf{Alan Turing Institute}
}
\date{October~8, 2020}

% Imported via UltiSnips
\input{global_macros}
\input{macros}

% Enable (uncolored) cross-reference hyperlinks
% Should always be last package loaded.
% See: https://tex.stackexchange.com/questions/103123/links-do-not-lead-to-right-pages
\usepackage{hyperref}
\let\oldhref\href
\renewcommand{\href}[2]{\oldhref{#1}{#2\hspace{0.05cm}{\scriptsize\faExternalLink}}}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Case Study}
\begin{frame}{Let's Start with a Case Study\ldots}
  \noindent
  Before examining the problem from a technical perspective, let's think through a \textit{real-world} scenario
  \begin{itemize}
    \item Help us identify salient themes
    \item Allow us to construct a working vocabulary
  \end{itemize}

  \vspace{18pt}
  \onslide<2->{
    \noindent
    \textbf{Note}: A case study appears at the end of paper, but I think the following will yield a more fruitful discussion.
  }
\end{frame}

\begin{frame}{Fair Criminal Sentencing}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{22pt}
    \item<+-> I get this is a bit heavy topic\ldots

    \item<+-> \textbf{Sentence}: Punishment (e.g.,~fine, probation, imprisonment, restitution) assigned in relation to a specific criminal act.

    \item<+-> \green{\textbf{Accurate}}/appropriate sentencing is \textbf{critical} for a healthy and fair justice system.

    \onslide<+->{%
      \item \textbf{Key Attributes of an Appropriate Sentencing Apparatus}
      \begin{itemize}
        \setlength{\itemsep}{3pt}
        \item \textit{Consistent}: Similar crimes/circumstances should yield similar punishments.
        \item \textit{Understandable}: Clarity \textit{why} the selected sentence is appropriate
        \item \textit{Transparent}: Knowledge of \textit{how} the sentence was selected sentence
        \item \textit{Sufficient}: Ensures justice for the victims and keeps the public safe
      \end{itemize}
    }
  \end{itemize}
\end{frame}

\begin{frame}{Problem with Sentencing Today}
  \onslide<+->{\textbf{\blue{Major Problem}}: Human juries/judges badly affected by bias\ldots}

  \onslide<+->{%
    \begin{itemize}
      \setlength{\itemsep}{22pt}
      \item \textbf{Racial Bias}: An African-American defendant is 5.9x as likely to be imprisoned as a white defendant. Hispanics are 3.1x as likely.~\citep{SentencingProject}

      \item \textbf{Gender Bias}: Men receive 63\% longer sentences on average than women do. Women are twice as likely to avoid incarceration if convicted.~\citep{Starr:2014}

      \item \textbf{Wealth Effects}: Poor defendants more likely to plead guilty since they cannot afford bail and lack a safety net to afford remaining in prison until trial
    \end{itemize}
  }
\end{frame}

\subsection{AI \& the Justice System}
\begin{frame}{Here's an Idea\ldots}
  \onslide<+->{Take the \textit{humans} out of sentencing
    \begin{itemize}
      \item Make it entirely ``data driven''
    \end{itemize}
  }

  \vspace{22pt}
  \onslide<+->{%
    \textbf{\blue{Idea}}: Use ML to estimate the \green{recidivism risk}. \textbf{Three types of recidivism}:
    \begin{itemize}
      \item \textit{Pretrial recidivism}
      \item \textit{General recidivism}
      \item \textit{Pretrial recidivism}
    \end{itemize}
  }

  \vspace{22pt}
  \onslide<+->{\blue{\textbf{COMPAS}}: \underline{C}orrectional \underline{O}ffender \underline{M}anagement \underline{P}rofiling for \underline{A}lternative \underline{S}anctions
    \begin{itemize}
      \item Released: 2009
      \item \green{\textbf{Proprietary}} (Developed by private company Northpointe)
    \end{itemize}
  }
\end{frame}

\begin{frame}{You May See Where This is Going\ldots}
  \begin{center}
    \onslide<2->{
      \includegraphics[scale=0.55]{propublica_compas_small.png}
    }
  \end{center}
\end{frame}

\begin{frame}{Some of the Highlights (or maybe \textit{Lowlights})\ldots}
  \noindent
  \onslide<+->{\blue{\textbf{Source}}: 2016 ProPublica expos\'{e}~\citep{Angwin:2016} (7~years after COMPAS' release)}

  \begin{itemize}[<+->]
    \setlength{\itemsep}{12pt}
    \item ``blacks are almost twice as likely as whites to be labeled a higher risk but not actually re-offend''

    \item ``makes the opposite mistake among whites: They are much more likely than blacks to be labeled lower-risk but go on to commit other crimes.''

    \item Only 20\% of people predicted to commit violent crimes actually went on to do so.

    \item ``More \green{accurate} than a human''~\citep{Dressel:2018}
      \begin{itemize}[<+->]
        \item Individual with \textit{no expertise} in Criminal Justice: 63\% accuracy
        \item COMPAS: 65\% accuracy
      \end{itemize}
  \end{itemize}

  \onslide<+->{
    \begin{block}{Takeaway}
      Explainable machine learning may have identified COMPAS's bias and flaws.
    \end{block}
  }
\end{frame}

\begin{frame}{Summary}
  \onslide<+->{
    \blue{\textbf{Key Themes of Practical Machine Learning}}:
      \begin{itemize}
        \setlength{\itemsep}{6pt}
        \item Proprietary IP vs.\ Transparency
        \item Model accuracy critical with very high stakes
        \item (Racial) bias
        \item Human understandability
        \item Importance of individual features (e.g.,~gender)
      \end{itemize}
  }

  \vspace{16pt}
  \onslide<+->{%
    \green{\textbf{Discussion Questions}}:
    \begin{itemize}
      \item What other high-level themes do you see in this example?
      \item Is there such a thing as a free lunch? Can I have it all?
    \end{itemize}
  }
\end{frame}

\section{Model Transparency}

\begin{frame}{Introduction}
  \begin{itemize}
    \setlength{\itemsep}{18pt}
    \item Machine learning systems are increasingly ubiquitous
    \item Understanding an ML system's decisions increases trust in those systems
    \item Many different stakeholders seek to understand different aspects of an ML model
      \begin{itemize}
        \item \textbf{Translation}: No one-size-fits-all explanation
      \end{itemize}
  \end{itemize}

  \onslide<2->{%
    \begin{block}{Purpose of this Paper}
      A broad survey of explainable ML to help \textit{industry practitioners} better understand the field and apply the right tools
    \end{block}
  }
\end{frame}

\section{Perspectives on Explainability}
\subsection{Model Transparency}

\begin{frame}{Properties of Transparent Models}{}
  \onslide<+->{
    \green{\textbf{Proprietary}} COMPAS is totally \textbf{\blue{opaque}}
    \begin{itemize}
      \item Major risk of corporate AI
    \end{itemize}
  }

  \vspace{20pt}
  \onslide<+->{\textbf{Levels of Model Transparency}}:
  \begin{itemize}[<+->]
    \setlength{\itemsep}{12pt}
    \item \green{\textbf{Simulatability}}: Can a human mentally simulate (i.e.,~calculate) the model output

    \item \green{\textbf{Decomposibility}}: Ability to break down a model into discrete (explainable) parts
      \begin{itemize}
        \item \textit{Example}: Input, parameters, computations, etc.
      \end{itemize}

    \item \green{\textbf{Algorithmic Transparency}}: Ability to understand the procedure the model uses to generate the output
      \begin{itemize}
        \item \textit{Example Transparent Model}: K-Nearest Neighbors
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Transparent to Opaque Models: A Fuzzy Continuum}{}
  The authors broadly categorize models as either:
    \begin{columns}[t]
      \begin{column}{0.025\textwidth}
      \end{column}
      \begin{column}{0.40\textwidth}
        \begin{center}
          \textbf{\large \green{Transparent}}
          \begin{itemize}
            \item Linear/logistic regression
            \item Decision tree
            \item K-nearest neighbors
            \item Rule-based learning
            \item Generalized additive models
            \item Bayesian networks
          \end{itemize}
        \end{center}
      \end{column}
      \begin{column}{0.05\textwidth}
      \end{column}
      \begin{column}{0.40\textwidth}
        \begin{center}
          \textbf{\large \green{Opaque}}
          \begin{itemize}
            \item Random forests
            \item Support vector machines
            \item Multi-layer (i.e.,~deep) neural networks
          \end{itemize}
        \end{center}
      \end{column}
      \begin{column}{0.025\textwidth}
      \end{column}
    \end{columns}

  \vspace{16pt}
  \onslide<2->{\blue{\textbf{Discussion}}: Going around the room, \green{pick a model class} (e.g.,~random forest) \& \green{explain} why it falls into its category}
\end{frame}

\begin{frame}{Salient Theme on Algorithmic Transparency}
  \blue{\textbf{No Free Lunch}}*:  Clear, consistent trade-off between \textit{model complexity} and \textit{transparency}

  \vspace{16pt}
  \textbf{\blue{What that Means in Practice}}: Gaining transparency/explainability often means sacrificing (some) performance

  \vspace{16pt}
  \onslide<+->{\textbf{\green{Question}}: Are all models from a ``transparent'' model class actually transparent?}
  \begin{itemize}[<+->]
    \item No.  \textit{Why}?
    \item Similarly, not all models from an ``opaque'' model class is opaque.
    \item There's some nuance here.  It's a fuzzy classification.
  \end{itemize}

\end{frame}

\subsection{Evaluation Criteria}

\begin{frame}{Criteria for Evaluating an Explainability Method}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{19pt}
    \item \green{\textbf{Comprehensibility}}: Are the extracted representations understandable to a human?
    \item \green{\textbf{Fidelity}}: How accurately do the extracted representations capture the opaque model?
    \item \green{\textbf{Accuracy}}: How well do the extracted representations predict unseen examples?
    \item \green{\textbf{Scalability}}: Method's ability to scale to high dimension/large (neural) network sizes
    \item \green{\textbf{Generality}}: Does the mechanism induce a special training procedure?
  \end{itemize}
\end{frame}

\subsection{Types of Explanations}

\begin{frame}{Visualization Explanation Types}
  \begin{center}
    \includegraphics[scale=0.177]{visual_representation_explanation_types.png}
  \end{center}
  \begin{flushright}
    {\vspace{-16pt} \tiny \textbf{Source}:~\citep{Arrieta:2020}}
  \end{flushright}
\end{frame}

\begin{frame}{Types of Explanations}{}
  {\small
    \begin{itemize}[<+->]
      \setlength{\itemsep}{14pt}
      \item \textbf{\green{Text Explanation}}: Using symbols (e.g.,~natural language text, or propositional symbols) to explain the model's behavior
      \item \textbf{\green{Visual Explanation}}: Generate visualization that facilitate understanding, e.g.,~heat map
      \item \textbf{\green{Local Explanation}}: Explain how a model operates in a certain area of interest, e.g.,~explain the prediction for a specific instance
      \item \textbf{\green{Explanations by Example}}: Extract \textit{representative instances} from the training dataset
      \item \textbf{\green{Explanation by Simplification}}: Approximate an \textit{opaque} model using a simpler, easier-to-interpret one
      \item \textbf{\green{Feature Relevance Explanations}}: Quantify the influence of each feature
    \end{itemize}
  }
\end{frame}

\begin{frame}{Visualization Explanation Types}
  \begin{center}
    \includegraphics[scale=0.177]{visual_representation_explanation_types.png}
  \end{center}
  \begin{flushright}
    {\vspace{-16pt} \tiny \textbf{Source}:~\citep{Arrieta:2020}}
  \end{flushright}
\end{frame}

\begin{frame}{Explanations by Example\onslide<5->{ -- \blue{Normative}}}
  \begin{columns}
    \begin{column}{0.48\textwidth}
      \begin{itemize}
        \setlength{\itemsep}{18pt}
        \item Common approach in human learning via \green{prototypical examples}
        \onslide<2->{
          \item \blue{\textbf{Example}}: \href{https://quickdraw.withgoogle.com/}{Google Quick, Draw!}~\citep{Cai:2019}
            \begin{itemize}
            \setlength{\itemsep}{8pt}
              \item Like charades or the old TV~show ``\href{https://en.wikipedia.org/wiki/Win,_Lose_or_Draw}{Win, Lose, or Draw}''
              \item Given a prompt and the user draws it
              \item AI tries to guess the picture
            \end{itemize}
        }
        \item<3-> Let's try it together. \textbf{For the third prompt, just draw \green{\textbf{avocado}} regardless of the instruction}
      \end{itemize}
    \end{column}
    \begin{column}{0.1\textwidth}
    \end{column}
    \begin{column}{0.45\textwidth}
      \onslide<4->{%
        \includegraphics[scale=0.20]{avocado_quick_draw.png}
      }
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Local Explanation by Example\only<3->{ -- \blue{Comparative}}}
  \blue{\textbf{Obvious Question}}: Why was this \textit{particular instance} mispredicted?

  \vspace{16pt}
  \begin{center}
    \onslide<2->{
          \includegraphics[scale=0.264]{avocado_alternates_quick_draw.png}
    }
  \end{center}
\end{frame}

\section{Explainability Approaches}

\transitionFrame{\LARGE \blue{Post-Hoc Explainability Approaches}: \\\vspace{6pt} Two Categories}

\subsection{Random-Forest Specific}
\begin{frame}{Random Forest Explainability Approaches}
  \noindent
  Random forests~(RF) are among the best performing ML algorithms for a wide variety of domains.

  \vspace{18pt}
  \noindent
  First we examine, post-hoc RF explainability algorithms
  \begin{itemize}
    \item \green{\textbf{Post-Hoc}}: Refers to the application of explainability methods \textit{after} model training~\citep{Molnar:2019}
  \end{itemize}
\end{frame}

\begin{frame}{Explainability by Simplification: Rule Extraction}
  A decision tree can be viewed as a set of ordered rules
  \begin{itemize}
    \item RF simplification methods seek to extract a compact set of \textit{simple} rules from these complex models.
  \end{itemize}

  \vspace{16pt}
  \noindent
  \green{\textbf{Various Ideas}}:
  \begin{itemize}
    \setlength{\itemsep}{10pt}
    \item Convert an RF into a single decision tree
    \item Focus on extract \textit{more representative rules} by focusing on those that are most prominent
    \item Use local search (e.g.,~hill climbing) to construct a set of rules approximating the RF
  \end{itemize}
\end{frame}

\begin{frame}{Feature Relevance}
  \green{\textbf{Extensive Previous Work}} -- Topics include:
  \begin{itemize}
    \setlength{\itemsep}{15pt}
    \item Assess importance of individual features by computing how much the model's accuracy drops after excluding the feature
    \item Estimate the importance of a feature by measuring how much it needs to change to affect the model's prediction.  \textit{Intuition}: Small change, high importance
    \item Determine which subsets of features interact together
  \end{itemize}
\end{frame}

\subsection{Model Agnostic}

\begin{frame}{Model Agnostic Explainability Approaches}
  \textbf{\blue{Primary Advantage}}: Generally applicable and flexible

  \vspace{18pt}
  \textbf{\blue{Primary Disadvantage}}: Cannot utilize a model's distinct to produce better explanations
  \begin{itemize}
    \item Slower
    \item Lower fidelity
    \item Provides limited to no insight into the operation of the learning algorithm itself
  \end{itemize}
\end{frame}

\begin{frame}{Explainability by Simplification: LIME}
  \noindent
  \textbf{\blue{Greatest Hit}}: \underline{L}ocal \underline{I}interpretable \underline{M}odel-Agnostic \underline{E}xplanations~\citep{Ribeiro:2016}
  \begin{itemize}
    \item Approximates an opaque model locally via a transparent model (e.g.,~linear model or decision tree)
  \end{itemize}

  \vspace{18pt}
  \noindent
  \textbf{Thought Experiment}: Create a model trying to identify the type of animal in a picture
  \begin{itemize}
    \item Necessitates high level features
    \item LIME transforms input data into \textit{interpretable representations} understandable by humans
  \end{itemize}
\end{frame}

\begin{frame}{Counterfactuals}
  \begin{itemize}
    \setlength{\itemsep}{18pt}
    \item Instance as close as possible to an instance of interest but for which the model classifies the alternate instance differently

    \item \textit{\green{Example}}: Application is denied a loan.  A counterfactual could say which factors, if changed, would have led to the loan application being improved.
  \end{itemize}
\end{frame}

\begin{frame}{Feature Relevance: SHAP}
  \noindent
  \textbf{\blue{Greatest Hit}}: \underline{Sh}apley \underline{A}dditive ex\underline{P}lanations~\citep{Lundberg:2017}
  \begin{itemize}
    \item \textbf{\green{Shapley Value} of a Feature}: Average, expected marginal contribution to the model's decision
    \item Strong mathematical foundation in conditional game theory
  \end{itemize}

  \vspace{18pt}
  \noindent
  \textbf{Basic Objective}: Train a linear model around the instance to be explained
  \begin{itemize}
    \item Linear models make the weight of each feature obvious
  \end{itemize}

  \vspace{18pt}
  \noindent
  \textbf{Strong Assumption}: Each feature is independent
\end{frame}

\begin{frame}{Visual Explanation: ICE}
  % \begin{columns}
  %   \begin{column}{0.5\textwidth}
      \textbf{\blue{Greatest Hit}}: \underline{I}ndividual \underline{C}onditional \underline{E}xpectation~\citep{Goldstein:2015}

      \begin{itemize}
        \setlength{\itemsep}{10pt}
        \item Operates on the instance level
        \item Depicts model decision boundary as a function of a single feature
      \end{itemize}

      \vspace{8pt}
      \begin{center}
        \includegraphics[scale=0.2]{ice_plots.png}
      \end{center}
    % \end{column}
  % \end{columns}
\end{frame}

\section{Conclusions}

\begin{frame}{Good (Ordered) Rules of Thumb}{}
  \begin{enumerate}
    \setlength{\itemsep}{16pt}
    \item If explainability is essential for the application, try a transparent model first.
    \item If a transparent model does not perform well enough and \textit{you lose the explainability benefit}, use an opaque model.
    \item Employ a \textit{feature-relevance method} to get an instance-specific estimate of feature influence.
    \item Use a \textit{model simplification approach} to see if the locally important features are globally influential.
    \item Use a visualization technique to plot the decision boundary as a function of important features
  \end{enumerate}
\end{frame}

\begin{frame}{Closing Thoughts}{}
  \textbf{Audience First}: The ``best'' explanation depends on the needs/requirements of the individual audience.

  \vspace{18pt}
  \textbf{Try Diverse Methods}: Relying one algorithm/explanation type only gives a partial picture

  \vspace{18pt}
  \textbf{Be Safe}: Like all machine learning systems, XAI is vulnerable to adversarial attacks.
\end{frame}

\begin{frame}[noframenumbering]{Table of Contents}
  \tableofcontents
\end{frame}

\begin{frame}[noframenumbering,allowframebreaks]{Bibliography}{}
  {\tiny
    \frametitle{References}
    \bibliography{bib/ref.bib}
    \bibliographystyle{unsrtnat}
  }
\end{frame}
\end{document}
